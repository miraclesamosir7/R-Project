---
title: "Campus Recruitment | Kaggle"
author: "Miracle Samuel Samosir"
date: '2022-05-19'
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Classification Project
We will predict the placement status by some of the factors that may or may not influence the placement. So on this project we will answer some of the question:

1. Which factor influenced a candidate in getting placed?
2. Does percentage matters for one to get placed?
3. Which degree specialization is much demanded by corporate?

# Importing
## Importing Libraries
We import useful libraries such as dplyr for data manipulation, ggplot2 for visualizing data, rpart for decision tree modeling, and e1071 for naive bayes classifier.
```{r, message=FALSE, warning = FALSE}
library(dplyr)
library(ggplot2)
library(rpart)
library(e1071)
```

## Importing Data
The data was downloaded from https://www.kaggle.com/datasets/benroshan/factors-affecting-campus-placement
```{r cars}
data <- read.csv("C:\\Users\\User\\Downloads\\archive\\Placement_Data_Full_Class.csv")
head(data)
```

# EDA
## Variables and their relationship.
i) Work status by gender
```{r pressure, echo=FALSE, message = FALSE}
by_gender_status <- data %>% 
  select(gender, status) %>% 
  group_by(gender, status) %>% 
  count() %>% 
  summarise(n = n, percentage = ifelse(gender == 'M', n/length(which(data$gender == 'M')), 
            n/length(which(data$gender == 'F'))))
by_gender_status
```

ii) Work status by Degree Specialization and Gender
```{r}
special <- data %>% 
  select(specialisation, gender, status) %>% 
  filter(status == 'Placed') %>% 
  group_by(specialisation, gender, status) %>% 
  count() %>% 
  summarise(n, percentage = ifelse(specialisation == 'Mkt&Fin', n/length(which(data$specialisation == 'Mkt&Fin')), 
            n/length(which(data$specialisation == 'Mkt&HR'))))
special
```


## Visualization
i) Work status by gender
```{r}
by_gender_status_vis <- ggplot(by_gender_status, aes(gender,n, fill = status)) +
  geom_col(position = "dodge") + 
  xlab("Gender") + 
  ylab("Count") + 
  ggtitle("Placement Status by Gender") +
  scale_fill_discrete("Status") +
  theme_dark()
by_gender_status_vis
```

ii) Work status by employee specialisation
```{r}
by_specialisation <- ggplot(special, aes(gender, n, fill = status)) +
  geom_col(position = "Dodge") + ggtitle("Work Status by Specialisation and Gender") + 
  xlab("Specialisation") + ylab("Count") +
  scale_fill_discrete("Status") + facet_wrap(~specialisation) + theme_dark()
by_specialisation
```

# Pre-processing
## Creating function to check NA values in every column of the data
We then define a function that will count the NA values (if exist) in every column in our data
```{r}
column_names = c()
na_count = c()
check_na <- function(data){
  for (i in 1:length(data)){
    column_names = c(colnames(data[i]),column_names)
    na_count = c(sum(is.na(data[i])),na_count)
    na_val = data.frame(column_names,na_count)
  }
  print(na_val)
}
check_na(data)
```
# Modeling
## Preparing data for modelling
i) Data Structure
```{r}
str(data)
```
We see that the data is not ready for modeling because of the different data types such as string and also some of unused column for modeling. Therefore we need to change them to dummy variables (set them as factor) and remove some of the columns. The code for doing that is presented below:

ii) Data Manipulation
```{r}
data_model <- data %>% 
  select(-sl_no, -salary) %>% 
  mutate(gender = as.integer(as.factor(gender)), ssc_b = as.integer(as.factor(ssc_b)), 
         hsc_b = as.integer(as.factor(hsc_b)), hsc_s = as.integer(as.factor(hsc_s)), 
         degree_t = as.integer(as.factor(degree_t)), workex = as.integer(as.factor(workex)), 
         specialisation = as.integer(as.factor(specialisation)),
         status = as.integer(as.factor(status)))
str(data_model)
```
As we see, the value in the column of the dataset already changed, so the data is ready to use.

## Train Test Split
Before modeling, we will define a function that will split the data into train and test data. The train data will be use to train the model and the test data will be use to validate the result later after the modeling process. We split 80% of the data for training and the rest for testing.

```{r}
create_train_test <- function(data, size = 0.8, train = TRUE) {
  n_row = nrow(data)
  total_row = size * n_row
  train_sample <- 1:total_row
  if (train == TRUE) {
    return (data[train_sample, ])
  } else {
    return (data[-train_sample, ])
  }
}

train <- create_train_test(data_model, train = T)
test <- create_train_test(data_model, train = F)
```

# Classification
## 1) Decision Tree
After splitting the data into training and testing, we create a decision tree model using the training data. Then we will predict with the testing data. Lastly we will use accuracy as the performance metrics for the model.
```{r}
model <- rpart(status~., train, method = 'class')
prediction <- predict(model, test, type = 'class')
matrix <- table(test$status, prediction)
acc <- sum(diag(matrix))/sum(matrix)
print(paste('The Accuracy of the model is', round(acc,4)))
```
The decision tree model successfully predict 72% of the test data output.

## 2) Naive Bayes
We might want to try another classification algorithm for modeling that might give better results. So we will use naive bayes classifier.
```{r}
model2 <- naiveBayes(status~., train)
prediction2 <- predict(model2, test)
matrix2 <- table(test$status, prediction2)
acc2 <- sum(diag(matrix2))/sum(matrix2)
print(paste('The Accuracy of the model is', round(acc2,4)))
```

The naive bayes model successfully predict almost 70% of the test data output.

# Conclusion
Because our Decision Tree model has a higher accuracy, we will use the model to answer the question.

## 1) Which factor influenced a candidate in getting placed?
```{r}
model$variable.importance
```
To determine which of the variable that influence the model the most, we see the score below of the variable. As the result above, we see that there is 4 variable that influence the most. If we sort from the highest to the lowest it will be ssc_p > hsc_p > degree_p > mba_p. The secondary percentage is the factor that influence the work status (placed/not placed) the most, followed by highschool percentage, degree percentage and the last is the mba percentage.

## 2) Does percentage matters for one to get placed?

As we see on number 1 answer, score percentage is the factor that influence the most. So it matters.

## 3) Which degree specialization is much demanded by corporate?
```{r}
by_specialisation
```

As the graphic shows above, the most demanded specialisation by corporate is Mkt&Fin. Both man and woman have a higher number in Mkt&Fin than in Mkt&HR.
